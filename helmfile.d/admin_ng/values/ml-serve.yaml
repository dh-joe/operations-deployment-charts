# cluster_group is used to identify a group of similar clusters (like for one in eqiad and one in codfw)
# that share some config (values) in "admin_ng/values/<cluster_group>.yaml"
cluster_group: ml-serve

# Defaults applied to all ML clusters
GlobalNetworkPolicies:
  # Allow icmp for all pods and all directions. Useful in debugging
  allow-all-icmp:
    namespaceSelector: 'has(projectcalico.org/name) && projectcalico.org/name != "kube-system"'
    types:
      - Ingress
      - Egress
    ingress:
      - action: Allow
        protocol: ICMP
      - action: Allow
        protocol: ICMPv6
    egress:
      - action: Allow
        protocol: ICMP
      - action: Allow
        protocol: ICMPv6
  default-deny:
    namespaceSelector: 'has(projectcalico.org/name) && projectcalico.org/name != "kube-system"'
    types:
      - Ingress
      - Egress
    egress:
      # Allow all namespaces to communicate to DNS pods
      - action: Allow
        protocol: UDP
        destination:
          selector: 'k8s-app == "kube-dns"'
          ports:
            - 53
  # This allows egress from all pods to all pods. Ingress still needs to be allowed by the destination, though.
  allow-pod-to-pod:
    namespaceSelector: 'has(projectcalico.org/name) && projectcalico.org/name != "kube-system"'
    types:
      - Egress
    egress:
      - action: Allow
        destination:
          nets:
            # eqiad
            - "10.67.16.0/21"
            # codfw
            - "10.194.16.0/21"
      - action: Allow
        destination:
          nets:
            # eqiad
            - "2620:0:861:300::/64"
            # codfw
            - "2620:0:860:300::/64"

# Context: https://knative.dev/docs/serving/tag-resolution/
docker:
  registry_cidrs:
      - '10.2.2.44/32'
      - '10.2.1.44/32'

deployExtraClusterRoles:
  - "kserve"
  - "liftwing-debugging"

# List all namespaces that should be created in every ML Serve cluster.
# For info about what overrides are available, please check ./common.yaml.
namespaces:
  kube-system:
    systemNamespace: true
    allowCriticalPods: true
    pspClusterRole: allow-privileged-psp
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  istio-system:
    systemNamespace: true
    allowCriticalPods: true
    labels:
      istio-injection: disabled
  knative-serving:
    systemNamespace: true
    allowCriticalPods: true
    deployTLSCertificate: true
    # See helmfile_namespace_certs.yaml for more info
    # The helmfile config deploys, by default, a TLS cert
    # with hostname == namespace for every non-system namespace.
    # In our case, we have only one istio TLS config that is deployed
    # via knative-serving.
    # Note: We rely on ChangeProp to call LiftWing upon certain
    # Kafka events are emitted. Due to how nodejs works
    # (see https://github.com/nodejs/node/issues/37104) it is
    # not possible to call our inference endpoints with
    # custom HTTP Host headers, unless there are some special
    # SANs in the TLS certificate to support them.
    tlsHostnames:
      - inference
    tlsExtraSANs:
      - '*.revscoring-editquality-goodfaith.wikimedia.org'
      - '*.revscoring-editquality-damaging.wikimedia.org'
      - '*.revscoring-editquality-reverted.wikimedia.org'
      - '*.revscoring-draftquality.wikimedia.org'
      - '*.revscoring-drafttopic.wikimedia.org'
      - '*.revscoring-articlequality.wikimedia.org'
      - '*.revscoring-articletopic.wikimedia.org'
      - '*.experimental.wikimedia.org'
      - '*.articletopic-outlink.wikimedia.org'
      - '*.article-descriptions.wikimedia.org'
      - '*.revertrisk.wikimedia.org'
      - '*.llm.wikimedia.org'
    labels:
      istio-injection: disabled
  kserve:
    systemNamespace: true
    allowCriticalPods: true
    labels:
      control-plane: kserve-controller-manager
      controller-tools.k8s.io: "1.0"
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  cert-manager:
    systemNamespace: true
    allowCriticalPods: true
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  external-services:
    systemNamespace: true
    deployTLSCertificate: false
  revscoring-editquality-goodfaith:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "300"
          memory: "240Gi"
        limits:
          cpu: "300"
          memory: "240Gi"
  revscoring-editquality-damaging:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "300"
          memory: "240Gi"
        limits:
          cpu: "300"
          memory: "240Gi"
  revscoring-editquality-reverted:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revscoring-draftquality:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revscoring-articlequality:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "120"
          memory: "100Gi"
        limits:
          cpu: "120"
          memory: "100Gi"
  revscoring-articletopic:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revscoring-drafttopic:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  articletopic-outlink:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  revertrisk:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  readability:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  article-descriptions:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    limitranges:
      container:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "40Gi"
          cpu: "16"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "44Gi"
          cpu: "24"
  ores-legacy:
    deployClusterRole: deploy
    deployTLSCertificate: true
    # NOTE: istio-injection is disabled here because we want to keep comparibility
    # with servivesd deployed to wikikube (that don't use istio-proxy but tls-proxy).
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
    tlsExtraSANs:
      - 'ores.wikimedia.org'
      - 'ores-legacy.wikimedia.org'
  recommendation-api-ng:
    deployClusterRole: deploy
    deployTLSCertificate: true
    # NOTE: istio-injection is disabled here because we want to keep comparibility
    # with servivesd deployed to wikikube (that don't use istio-proxy but tls-proxy).
    labels:
      istio-injection: disabled
      webhooks.knative.dev/exclude: "true"
  llm:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
  # The experimental namespace allows the deployment of model servers
  # that are not ready for production yet. After an incubation period,
  # a model server may be promoted to production in its own dedicated
  # k8s namespace.
  experimental:
    deployClusterRole: deploy-kserve
    deployTLSCertificate: false
    labels:
      istio-injection: enabled
    limitranges:
      container:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "40Gi"
          cpu: "16"
        defaultRequest:
          memory: "100Mi"
          cpu: "100m"
        default:
          memory: "100Mi"
          cpu: "100m"
      pod:
        min:
          memory: "100Mi"
          cpu: "100m"
        max:
          memory: "44Gi"
          cpu: "24"
    resourcequota:
      pods: {}
      compute:
        requests:
          cpu: "90"
          memory: "150Gi"
        limits:
          cpu: "90"
          memory: "150Gi"

net_istio:
  ingressgateway:
    servers:
    # The 'hosts' field correspond to the list of backend routes that
    # Knative/Istio will allow/configure on the Gateway. For example,
    # if the FQDN for a Kfserving backend is something-test.example.com,
    # we can allow it in multiple ways:
    # 1) More specific: 'something-test.example.com'
    # 2) Less specific: '*.example.com'
    # 3) All traffic allowed: '*'
    # For the moment option 3) is fine, but we'll need to review the choice.
    - hosts:
        - '*'
      port:
        name: https
        number: 443
        protocol: HTTPS
      tls:
        mode: SIMPLE
        minProtocolVersion: 'TLSV1_2'
        # Please note:
        # This corresponds to the name of a TLS Secret deployed
        # to the istio-system namespace. We deploy it via
        # helmfile_namespace_certs.yaml.
        credentialName: knative-serving-tls-certificate
  mesh:
    service_entries:
    - name: mediawiki-api-ro
      spec:
        hosts:
        - api-ro.discovery.wmnet
        ports:
        - name: http-port
          # This port is used by the Destination Rule for TLS egress origination
          number: 80
          protocol: HTTP
          targetPort: 443
        - name: https-port
          number: 443
          protocol: HTTPS
        resolution: DNS
    - name: eventgate-main-se
      spec:
        hosts:
        - eventgate-main.discovery.wmnet
        ports:
        - name: http-port
          # This port is used by the Destination Rule for TLS egress origination
          number: 4480
          protocol: HTTP
          targetPort: 4492
        - name: https-port
          number: 4492
          protocol: HTTPS
        resolution: DNS
    - name: rest-gateway
      spec:
        hosts:
        - rest-gateway.discovery.wmnet
        ports:
        - name: http-port
          # This port is used by the Destination Rule for TLS egress origination
          number: 4111
          protocol: HTTP
          targetPort: 4113
        - name: https-port
          number: 4113
          protocol: HTTPS
        resolution: DNS
    destination_rules:
    - name: https-api-ro
      spec:
        host: api-ro.discovery.wmnet
        trafficPolicy:
          # The configuration is related to TLS egress origination.
          # See https://istio.io/v1.9/docs/tasks/traffic-management/egress/egress-tls-origination/#tls-origination-for-egress-traffic
          # The TLS settings will force istio-proxy to pick up the HTTPS port
          # set in the related ServiceEntry.
          portLevelSettings:
          - port:
              number: 80
            tls:
              mode: SIMPLE
          connectionPool:
            tcp:
              maxConnections: 100
              connectTimeout: 30s
            http:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              idleTimeout: 5s
              maxRequestsPerConnection: 1000
              # See retries implemented in the Virtual Service settings
              maxRetries: 0
    - name: https-eventgate-dr
      spec:
        host: eventgate-main.discovery.wmnet
        trafficPolicy:
          # The configuration is related to TLS egress origination.
          # See https://istio.io/v1.9/docs/tasks/traffic-management/egress/egress-tls-origination/#tls-origination-for-egress-traffic
          # The TLS settings will force istio-proxy to pick up the HTTPS port
          # set in the related ServiceEntry.
          portLevelSettings:
          - port:
              number: 4480
            tls:
              mode: SIMPLE
          connectionPool:
            tcp:
              maxConnections: 50
              connectTimeout: 30s
            http:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              idleTimeout: 5s
              maxRequestsPerConnection: 1000
              # See retries implemented in the Virtual Service settings
              maxRetries: 0
    - name: https-rest-gateway
      spec:
        host: rest-gateway.discovery.wmnet
        trafficPolicy:
          # The configuration is related to TLS egress origination.
          # See https://istio.io/v1.9/docs/tasks/traffic-management/egress/egress-tls-origination/#tls-origination-for-egress-traffic
          # The TLS settings will force istio-proxy to pick up the HTTPS port
          # set in the related ServiceEntry.
          portLevelSettings:
          - port:
              number: 4111
            tls:
              mode: SIMPLE
          connectionPool:
            tcp:
              maxConnections: 100
              connectTimeout: 30s
            http:
              # Rationale (may need to be revisited after upgrading Istio):
              # https://phabricator.wikimedia.org/T320374#8338627
              idleTimeout: 5s
              maxRequestsPerConnection: 1000
              # See retries implemented in the Virtual Service settings
              maxRetries: 0
    virtual_services:
    - name: mediawiki-api-vs
      spec:
        gateways:
        - mesh
        hosts:
        - '*.wikipedia.org'
        - 'wikipedia.org'
        - '*.wiktionary.org'
        - 'wiktionary.org'
        - '*.wikidata.org'
        - 'wikidata.org'
        - '*.wikiquote.org'
        - 'wikiquote.org'
        - '*.wikibooks.org'
        - 'wikibooks.org'
        - '*.wikisource.org'
        - 'wikisource.org'
        http:
        - match:
          - port: 80
          route:
          - destination:
              host: api-ro.discovery.wmnet
          retries:
            # Rationale (may need to be revisited after upgrading Istio):
            # https://phabricator.wikimedia.org/T320374#8338627
            attempts: 3
            retryOn: connect-failure,refused-stream,reset,503
          headers:
            request:
              set:
                # The TLS egress origination settings (see the related destination
                # rule's comment) have the side effect of presenting
                # a 'x-forwarded-proto: http' header to the target API, that in turn
                # may return a warning in every response. To circumvent this problem,
                # let's explicitly set the header.
                x-forwarded-proto: https
    - name: eventgate-main-vs
      spec:
        gateways:
        - mesh
        hosts:
        - 'eventgate-main.discovery.wmnet'
        http:
        - match:
          - port: 4480
          route:
          - destination:
              host: eventgate-main.discovery.wmnet
          retries:
            # Rationale (may need to be revisited after upgrading Istio):
            # https://phabricator.wikimedia.org/T320374#8338627
            attempts: 3
            retryOn: connect-failure,refused-stream,reset,503
          headers:
            request:
              set:
                # The TLS egress origination settings (see the related destination
                # rule's comment) have the side effect of presenting
                # a 'x-forwarded-proto: http' header to the target API, that in turn
                # may return a warning in every response. To circumvent this problem,
                # let's explicitly set the header.
                x-forwarded-proto: https
    - name: restgw-vs
      spec:
        gateways:
        - mesh
        hosts:
        - '*.wikipedia.org'
        - 'wikipedia.org'
        http:
        - match:
          - port: 4111
          route:
          - destination:
              host: rest-gateway.discovery.wmnet
          retries:
            # Rationale (may need to be revisited after upgrading Istio):
            # https://phabricator.wikimedia.org/T320374#8338627
            attempts: 3
            retryOn: connect-failure,refused-stream,reset,503
          headers:
            request:
              set:
                # The TLS egress origination settings (see the related destination
                # rule's comment) have the side effect of presenting
                # a 'x-forwarded-proto: http' header to the target API, that in turn
                # may return a warning in every response. To circumvent this problem,
                # let's explicitly set the header.
                x-forwarded-proto: https

core:
  config_features:
    # This is needed to be able to configure the Pod's dnsConfig
    # from Kserve's InferenceService resources.
    kubernetes.podspec-dnsconfig: "enabled"

helmVersion: "helm3"

istio:
  gateways:
    ingressgateway:
      ports:
        - 8443
        # knative-local-gateway (not exposed via NodePort),
        # only for internal traffic.
        - 8081
  sidecar:
    rate_limit_configs:
      - name: "filter-local-ratelimit-svc"
        spec:
          workloadSelector:
            labels:
              app-wmf: kserve-inference
          configPatches:
            - applyTo: HTTP_FILTER
              match:
                context: SIDECAR_INBOUND
                listener:
                  filterChain:
                    filter:
                      name: "envoy.filters.network.http_connection_manager"
              patch:
                operation: INSERT_BEFORE
                value:
                  name: envoy.filters.http.local_ratelimit
                  typed_config:
                    "@type": type.googleapis.com/udpa.type.v1.TypedStruct
                    type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
                    value:
                      stat_prefix: http_local_rate_limiter
                      token_bucket:
                        max_tokens: 100
                        tokens_per_fill: 100
                        fill_interval: 1s
                      filter_enabled:
                        runtime_key: local_rate_limit_enabled
                        default_value:
                          numerator: 100
                          denominator: HUNDRED
                      filter_enforced:
                        runtime_key: local_rate_limit_enforced
                        default_value:
                          numerator: 100
                          denominator: HUNDRED
                      response_headers_to_add:
                        - append: false
                          header:
                            key: x-local-rate-limit
                            value: 'true'


# Measure to deal with https://phabricator.wikimedia.org/T318814
# By default Kubernetes DNS records have 5s TTL, and in a Service Mesh
# like Istio this means causing a lot of DNS queries from Envoy to refresh
# every TTL seconds all the Clusters with STRICT_DNS settings.
coredns:
  rewrite_actions:
    continue:
    - 'ttl exact knative-local-gateway.istio-system.svc.cluster.local. 30'
    - 'ttl regex (.*).discovery.wmnet 30'
    - 'ttl regex (.*)-(predictor|transformer)-default-(.*) 30'

kserve:
  ingress:
    local_gateway_service: knative-local-gateway.istio-system.svc.cluster.local
    domain: 'wikimedia.org'

limitranges:
  container:
    max:
      memory: "8Gi"
  pod:
    max:
      memory: "10Gi"

typha:
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: ~
      memory: 200Mi

kubeControllers:
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: ~
      memory: 200Mi
