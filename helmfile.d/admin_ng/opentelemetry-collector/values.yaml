mode: daemonset

rollout:
  rollingUpdate:
    maxUnavailable: 3

# The Kubernetes Attributes Processor automatically discovers Kubernetes pods, extracts their metadata,
# and adds the extracted metadata to spans, metrics, and logs as resource attributes.
# https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-attributes-processor
# https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor/README.md
presets:
  kubernetesAttributes:
    enabled: true

config:
  # Disable all receivers except OTLP
  receivers:
    jaeger: null
    prometheus: null
    zipkin: null
  exporters:
    otlp:
      # Don't forget to update the egress rules below upon changing this
      endpoint: jaeger-collector-grpc.svc.eqiad.wmnet:30443
      tls:
        ca_file: /etc/ssl/certs/wmf-ca-certificates.crt
  processors:
    # Rewrite service.name to something meaningful, as our Envoys don't
    # currently provide this.  https://phabricator.wikimedia.org/T363407
    # Do this using the transformprocessor:
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor
    #
    # In this step, because of https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/32080#issuecomment-2112490125
    # we will only mutate the per-span `service.name` attribute, not the resource-level one.
    # Later we will use groupbyattrs to aggregate these into the Resource's service.name correctly.
    transform/service_from_upstream_cluster_or_nodeid:
      error_mode: ignore
      trace_statements:
        - context: span
          statements:
            # Begin by duplicating any resource service.name into the span-level attributes.
            - set(attributes["service.name"], resource.attributes["service.name"])
            # If we have a node_id that looks pod-name-esque, try to parse out
            # something we can use as a service name from the first part.
            # (Really it will be the k8s namespace.)
            - set(attributes["_k8s_namespace"], attributes["node_id"])
                where (attributes["node_id"] != nil) and (attributes["node_id"] != "")
            # Yes, unfortunately, you do need to escape $ as $$, because otelcol performs
            # environment variable substitution in its parsed configuration.
            # Also remember to write \\ to get a single backslash as output of the YAML parser.
            - replace_pattern(attributes["_k8s_namespace"], "^([^.]+)\\..*$$", "$$1")
            # Remove any "LOCAL_" prefix from the upstream cluster name...
            - replace_pattern(attributes["upstream_cluster.name"], "^LOCAL_", "")
            # ...and potentially use the rest as the service name, if we don't have a good one already.
            - set(attributes["service.name"], attributes["upstream_cluster.name"])
                where ((attributes["service.name"] == nil)
                       or (attributes["service.name"] == "")
                       or (attributes["service.name"] == "OTLPResourceNoServiceName"))
                and (attributes["upstream_cluster.name"] != "local_service")
            # Failing that, use the node_id-derived service name.
            - set(attributes["service.name"], attributes["_k8s_namespace"])
                where ((attributes["service.name"] == nil)
                       or (attributes["service.name"] == "")
                       or (attributes["service.name"] == "OTLPResourceNoServiceName"))
                and (attributes["_k8s_namespace"] != nil) and (attributes["_k8s_namespace"] != "")
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md
    groupbyattrs/service_name:
      keys:
        - service.name
  service:
    pipelines:
      metrics:
        receivers:
          - otlp
      traces:
        receivers:
          - otlp
        processors:
          - transform/service_from_upstream_cluster_or_nodeid
          - groupbyattrs/service_name
          - memory_limiter
          - batch
        exporters:
          - otlp

resources:
  limits:
    cpu: 256m
    memory: 512Mi

# Use the WMF Docker registry, not Docker Hub, and specify our most recent build (overriding the
# appVersion from Chart.yaml).
image:
  repository: docker-registry.discovery.wmnet/otelcol
  tag: v0.100.0-1

command:
  # The chart adds a leading slash (because the default is just "otelcol-contrib" with the binary
  # installed at the root.)
  name: usr/bin/otelcol-contrib

# Disable all ports except OTLP
ports:
  otlp:
    # Disable the hostPort; we'll use a NodePort service instead.
    hostPort: null
  otlp-http:
    # Disable the hostPort; we'll use a NodePort service instead.
    hostPort: null
  jaeger-compact:
    enabled: false
  jaeger-thrift:
    enabled: false
  jaeger-grpc:
    enabled: false
  zipkin:
    enabled: false

service:
  enabled: true
  type: ClusterIP
  internalTrafficPolicy: Local

networkPolicy:
  enabled: true
  egressRules:
    - to:
        - ipBlock:
            cidr: 10.2.2.78/32 # jaeger-collector-grpc.svc.eqiad.wmnet
      ports:
        - port: 30443
          protocol: TCP
    # TODO: Here, in order to support the k8sattributes processor, we hardcode the IPs of the
    # k8s API endpoints.  This is not ideal, but it's the best we can do until we have a better
    # way -- probably either creating a wrapper chart that also creates a Calico NetworkPolicy
    # object that allows the k8s API endpoints, or, creating an otelcol chart of our own.
    # https://phabricator.wikimedia.org/T365855
    # kubectl get endpoints kubernetes -oyaml | yq -c '.subsets[].addresses[] | {"ipBlock": {"cidr": (.ip + "/32")}}' - | jq -s | yq -y
    - to:
        # eqiad k8s API endpoint IPs
        - ipBlock:
            cidr: 10.64.0.117/32
        - ipBlock:
            cidr: 10.64.32.116/32
        # codfw k8s API endpoint IPs
        - ipBlock:
            cidr: 10.192.0.56/32
        - ipBlock:
            cidr: 10.192.15.6/32
        - ipBlock:
            cidr: 10.192.16.48/32
        - ipBlock:
            cidr: 10.192.32.105/32
        - ipBlock:
            cidr: 10.192.5.10/32
      ports:
        - port: 6443
          protocol: TCP

  extraIngressRules:
    - ports:
      - port: 8888
        protocol: TCP

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"
