mode: daemonset

rollout:
  rollingUpdate:
    maxUnavailable: 9

# The Kubernetes Attributes Processor automatically discovers Kubernetes pods, extracts their metadata,
# and adds the extracted metadata to spans, metrics, and logs as resource attributes.
# https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-attributes-processor
# https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor/README.md
presets:
  kubernetesAttributes:
    enabled: true

config:
  # Disable all receivers except OTLP
  receivers:
    jaeger: null
    prometheus: null
    zipkin: null
  exporters:
    otlp:
      # Don't forget to update the egress rules below upon changing this
      endpoint: jaeger-collector-grpc.svc.eqiad.wmnet:30443
      tls:
        ca_file: /etc/ssl/certs/wmf-ca-certificates.crt
  processors:
    # Override the default settings of the k8sattributes processor -- we extract fewer
    # fields, and don't extract any that require a LIST+WATCH on replicasets (T366094)
    k8sattributes:
      extract:
        metadata:
          - "k8s.namespace.name"
          - "k8s.node.name"
          - "k8s.pod.name"
          - "k8s.pod.uid"
          - "k8s.pod.start_time"

    # Rewrite service.name to something meaningful, as our Envoys don't
    # currently provide this.  https://phabricator.wikimedia.org/T363407
    # Do this using the transformprocessor:
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor
    #
    # In this step, because of https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/32080#issuecomment-2112490125
    # we will only mutate the per-span `service.name` attribute, not the resource-level one.
    # Later we will use groupbyattrs to aggregate these into the Resource's service.name correctly.
    transform/service_from_upstream_cluster_or_nodeid:
      error_mode: ignore
      trace_statements:
        - context: span
          statements:
            # Begin by duplicating any resource service.name into the span-level attributes.
            - set(attributes["service.name"], resource.attributes["service.name"])
            # Remove any "LOCAL_" prefix from the upstream cluster name...
            - replace_pattern(attributes["upstream_cluster.name"], "^LOCAL_", "")
            # ...and potentially use the rest as the service name, if we don't have a good one already.
            - set(attributes["service.name"], attributes["upstream_cluster.name"])
                where ((attributes["service.name"] == nil)
                       or (attributes["service.name"] == "")
                       or (attributes["service.name"] == "OTLPResourceNoServiceName"))
                and (attributes["upstream_cluster.name"] != "local_service")
            # Failing that, use the k8s-namespace-derived service name.
            - set(attributes["service.name"], resource.attributes["k8s.namespace.name"])
                where ((attributes["service.name"] == nil)
                       or (attributes["service.name"] == "")
                       or (attributes["service.name"] == "OTLPResourceNoServiceName"))
                and (resource.attributes["k8s.namespace.name"] != nil) and (resource.attributes["k8s.namespace.name"] != "")
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md
    groupbyattrs/service_name:
      keys:
        - service.name
  service:
    pipelines:
      metrics: ~
      logs: ~
      traces:
        receivers:
          - otlp
        processors:
          - k8sattributes
          - transform/service_from_upstream_cluster_or_nodeid
          - groupbyattrs/service_name
          - memory_limiter
          - batch
        exporters:
          - otlp

resources:
  limits:
    cpu: 256m
    memory: 512Mi

# Use the WMF Docker registry, not Docker Hub, and specify our most recent build (overriding the
# appVersion from Chart.yaml).
image:
  repository: docker-registry.discovery.wmnet/otelcol
  tag: 0.102.0-1

command:
  # The chart adds a leading slash (because the default is just "otelcol-contrib" with the binary
  # installed at the root.)
  name: usr/bin/otelcol-contrib

# Disable all ports except OTLP
ports:
  otlp:
    # Disable the hostPort; we'll use a NodePort service instead.
    hostPort: null
  otlp-http:
    # Disable the hostPort; we'll use a NodePort service instead.
    hostPort: null
  jaeger-compact:
    enabled: false
  jaeger-thrift:
    enabled: false
  jaeger-grpc:
    enabled: false
  zipkin:
    enabled: false

service:
  enabled: true
  type: ClusterIP
  internalTrafficPolicy: Local

networkPolicy:
  enabled: true
  egressRules:
    - to:
        - ipBlock:
            cidr: 10.2.2.78/32 # jaeger-collector-grpc.svc.eqiad.wmnet
      ports:
        - port: 30443
          protocol: TCP
    # TODO: Here, in order to support the k8sattributes processor, we hardcode the IPs of the
    # k8s API endpoints.  This is not ideal, but it's the best we can do until we have a better
    # way -- probably either creating a wrapper chart that also creates a Calico NetworkPolicy
    # object that allows the k8s API endpoints, or, creating an otelcol chart of our own.
    # https://phabricator.wikimedia.org/T365855
    # kubectl get endpoints -n default kubernetes -oyaml | yq -c '.subsets[].addresses[] | {"ipBlock": {"cidr": (.ip + "/32")}}' - | jq -s | yq -y
    - to:
        # eqiad k8s API endpoint IPs
        - ipBlock:
            cidr: 10.64.0.117/32
        - ipBlock:
            cidr: 10.64.32.116/32
        - ipBlock:
            cidr: 10.64.16.93/32
        - ipBlock:
            cidr: 10.64.48.45/32
        - ipBlock:
            cidr: 10.64.32.37/32
        # codfw k8s API endpoint IPs
        - ipBlock:
            cidr: 10.192.0.56/32
        - ipBlock:
            cidr: 10.192.15.6/32
        - ipBlock:
            cidr: 10.192.16.48/32
        - ipBlock:
            cidr: 10.192.32.105/32
        - ipBlock:
            cidr: 10.192.5.10/32
      ports:
        - port: 6443
          protocol: TCP

  extraIngressRules:
    - ports:
      - port: 8888
        protocol: TCP

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"
