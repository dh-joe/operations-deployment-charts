inference:
  predictor:
    config:
      # These settings are applied at the pod level,
      # that includes all containers not explicitly defined
      # in the isvc (like the istio-{validation,proxy}).
      # The knative-serving ones seem to need
      # an extra level of settings in the knative's control-plane
      # due to how the various pods/revisions are handled
      # (dynamically etc..).
      # More info https://phabricator.wikimedia.org/T369493
      securityContext:
        seccompProfile:
          type: RuntimeDefault

inference_services:
  revertrisk-wikidata:
    predictor:
      image: "machinelearning-liftwing-inference-services-revertrisk-wikidata"
      image_version: "2024-06-27-100336-publish"
      custom_env:
        - name: MODEL_NAME
          value: "revertrisk-wikidata"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/experimental/revertrisk-wikidata/20230512162400/"
        - name: WIKI_URL
          value: "http://mw-api-int-ro.discovery.wmnet:4680"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
          requests:
            cpu: "2"
            memory: 4Gi
  logo-detection:
    predictor:
      image: "machinelearning-liftwing-inference-services-logo-detection"
      image_version: "2024-07-08-163719-publish"
      custom_env:
        - name: MODEL_NAME
          value: "logo-detection"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/logo-detection/20240417132942/"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "2"
            memory: 2Gi

  aya:
      predictor:
        image: "machinelearning-liftwing-inference-services-huggingface"
        image_version: "2024-08-16-130843-publish"
        custom_env:
          - name: MODEL_NAME
            value: "aya-expanse-8B"
          - name: STORAGE_URI
            value: "s3://wmf-ml-models/llm/aya-expanse-8B/"
        container:
          command: ["./entrypoint.sh"]
          args: [ "--backend", "huggingface", "--dtype", "bfloat16" ]
          resources:
            limits:
              cpu: "8"
              memory: 35Gi
              amd.com/gpu: "1"
            requests:
              cpu: "8"
              memory: 35Gi
              amd.com/gpu: "1"

  aya-llm:
      predictor:
        image: "machinelearning-liftwing-inference-services-llm"
        image_version: "2025-01-03-150634-publish"
        custom_env:
          - name: MODEL_NAME
            value: "aya-expanse-8B"
          - name: STORAGE_URI
            value: "s3://wmf-ml-models/llm/aya-expanse-8B/"
          - name: LLM_CLASS
            value: "llm.Aya"
          - name: BITSANDBYTES_DTYPE
            value: "int4"
          - name: DEVICE
            value: "auto"
          - name: ATTN_IMPLEMENTATION
            value: "flash_attention_2"
          - name: DTYPE
            value: "bfloat16"
        container:
          resources:
            limits:
              cpu: "8"
              memory: 35Gi
              amd.com/gpu: "1"
            requests:
              cpu: "8"
              memory: 35Gi
              amd.com/gpu: "1"

  bert:
      predictor:
        image: "machinelearning-liftwing-inference-services-huggingface"
        image_version: "2024-05-23-145141-publish"
        custom_env:
          - name: MODEL_NAME
            value: "bert"
          - name: STORAGE_URI
            value: "s3://wmf-ml-models/llm/bert-base-uncased/"
        container:
          command: [ "python3", "-m", "huggingfaceserver", "--model_dir", "/mnt/models", "--model_name", "bert"]
          resources:
            limits:
              memory: 2Gi
            requests:
              memory: 2Gi

  articlequality:
    predictor:
      image: "machinelearning-liftwing-inference-services-articlequality"
      image_version: "2024-08-01-160129-publish"
      custom_env:
        - name: MODEL_NAME
          value: "articlequality"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/articlequality/language-agnostic/20240801111508/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            memory: 1Gi
          requests:
            memory: 1Gi

  reference-need:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 1
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-03-19-145037-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-need"
        - name: BATCH_SIZE
          value: "16"
          # We set number of threads for each worker model to (num_cpus/2)-1 to leave CPU capacity to the main event loop
        - name: NUM_THREADS
          value: "7"
        - name: NUM_OF_WORKERS
          value: "2"
      container:
        resources:
          limits:
            cpu: "16"
            memory: 4Gi
          requests:
            cpu: "16"
            memory: 4Gi

  reference-risk:
    annotations:
      autoscaling.knative.dev/target: "3"
    predictor:
      config:
        minReplicas: 1
        maxReplicas: 8
      image: "machinelearning-liftwing-inference-services-reference-quality"
      image_version: "2025-03-11-113532-publish"
      custom_env:
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/reference-quality/20250127142109/"
        - name: FORCE_HTTP
          value: "True"
        - name: MODEL_TO_DEPLOY
          value: "reference-risk"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 3Gi
          requests:
            cpu: "4"
            memory: 3Gi


  article-country:
    predictor:
      image: "machinelearning-liftwing-inference-services-article-country"
      image_version: "2024-12-09-142136-publish"
      custom_env:
        - name: MODEL_NAME
          value: "article-country"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/article-country/20240901015102/"
        - name: FORCE_HTTP
          value: "True"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "2"
            memory: 2Gi

  edit-check:
    predictor:
      batcher:
        maxBatchSize: 32
        maxLatency: 20
      config:
        maxReplicas: 1
      image: "machinelearning-liftwing-inference-services-edit-check"
      image_version: "2025-04-03-152545-publish"
      custom_env:
        - name: MODEL_NAME
          value: "edit-check-staging"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/edit-check/peacock/"
        - name: MODEL_VERSION
          value: "v1"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 8Gi
            amd.com/gpu: "1"
          requests:
            cpu: "4"
            memory: 8Gi
            amd.com/gpu: "1"

external_services_app_label_selector: app-wmf
external_services:
  cassandra:
  - ml-cache-a-eqiad
  - ml-cache-a-codfw
