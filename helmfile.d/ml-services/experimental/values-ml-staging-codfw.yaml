inference_services:
  revertrisk-wikidata:
    predictor:
      image: "machinelearning-liftwing-inference-services-revertrisk-wikidata"
      image_version: "2024-04-23-155719-publish"
      custom_env:
        - name: MODEL_NAME
          value: "revertrisk-wikidata"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/experimental/revertrisk-wikidata/20230512162400/"
        - name: WIKI_URL
          value: "http://mw-api-int-ro.discovery.wmnet:4680"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
          requests:
            cpu: "2"
            memory: 4Gi
  logo-detection:
    predictor:
      image: "machinelearning-liftwing-inference-services-logo-detection"
      image_version: "2024-05-16-145409-publish"
      custom_env:
        - name: MODEL_NAME
          value: "logo-detection"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/logo-detection/20240417132942/"
      container:
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "2"
            memory: 2Gi

  mistral-7b-instruct-gpu:
      predictor:
        image: "machinelearning-liftwing-inference-services-huggingface"
        image_version: "2024-05-23-145141-publish"
        custom_env:
          - name: MODEL_NAME
            value: "mistral-7b-instruct"
          - name: STORAGE_URI
            value: "s3://wmf-ml-models/llm/Mistral-7B-Instruct-v0.2/"
        container:
          command: [ "python3", "-m", "huggingfaceserver", "--model_dir", "/mnt/models", "--model_name", "mistral-7b-instruct", "--backend", "huggingface" ]
          resources:
            limits:
              cpu: "8"
              memory: 35Gi
              amd.com/gpu: "1"
            requests:
              cpu: "8"
              memory: 35Gi
              amd.com/gpu: "1"

  bert:
      predictor:
        image: "machinelearning-liftwing-inference-services-huggingface"
        image_version: "2024-05-23-145141-publish"
        custom_env:
          - name: MODEL_NAME
            value: "bert"
          - name: STORAGE_URI
            value: "s3://wmf-ml-models/llm/bert-base-uncased/"
        container:
          command: [ "python3", "-m", "huggingfaceserver", "--model_dir", "/mnt/models", "--model_name", "bert"]
          resources:
            limits:
              memory: 2Gi
            requests:
              memory: 2Gi

external_services_app_label_selector: app-wmf
external_services:
  cassandra:
  - ml-cache-a-eqiad
  - ml-cache-a-codfw
