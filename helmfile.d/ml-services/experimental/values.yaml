docker:
  registry: docker-registry.discovery.wmnet/wikimedia
  imagePullPolicy: IfNotPresent

networkpolicy:
  egress:
    enabled: true
    # These endpoints should be reachable by Istio proxy sidecars.
    dst_nets:
      - cidr: 10.2.1.54/32 # thanos-swift.svc.codfw.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.2.54/32 # thanos-swift.svc.eqiad.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.1.22/32 # api-ro.svc.codfw.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.2.22/32 # api-ro.svc.eqiad.wmnet
        ports:
        - port: 443
          protocol: tcp
      - cidr: 10.2.1.45/32 # eventgate-main.svc.codfw.wmnet
        ports:
        - port: 4492
          protocol: tcp
      - cidr: 10.2.2.45/32 # eventgate-main.svc.eqiad.wmnet
        ports:
        - port: 4492
          protocol: tcp

monitoring:
  enabled: true

inference:
  annotations:
    sidecar.istio.io/inject: "true"
    proxy.istio.io/config: '{"readiness.status.sidecar.istio.io/periodSeconds": "600" }'

inference_services:
  bloom-560m:
    predictor:
      image: "machinelearning-liftwing-inference-services-llm"
      image_version: "2023-06-09-090444-publish"
      custom_env:
        - name: MODEL_NAME
          value: "bloom-560m"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/bloom/bloom-560m/20230512141345/"
      container:
        resources:
          limits:
            cpu: "4"
            memory: 6Gi
          requests:
            cpu: "4"
            memory: 6Gi
  bloom-3b:
    predictor:
      image: "machinelearning-liftwing-inference-services-llm"
      image_version: "2023-06-09-090444-publish"
      custom_env:
        - name: MODEL_NAME
          value: "bloom-3b"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/bloom/bloom-3b/"
      container:
        resources:
          limits:
            cpu: "8"
            memory: 20Gi
          requests:
            cpu: "8"
            memory: 20Gi
  falcon-7b-instruct-gpu:
    predictor:
      image: "machinelearning-liftwing-inference-services-llm"
      image_version: "2023-06-09-090444-publish"
      custom_env:
        - name: MODEL_NAME
          value: "falcon-7b-instruct-gpu"
        - name: STORAGE_URI
          value: "s3://wmf-ml-models/llm/falcon-7b-instruct/"
      container:
        readinessProbe:
          initialDelaySeconds: 120
          periodSeconds: 600
        resources:
          limits:
            cpu: "8"
            memory: 40Gi
            amd.com/gpu: 1
          requests:
            cpu: "8"
            memory: 30Gi
            amd.com/gpu: 1
