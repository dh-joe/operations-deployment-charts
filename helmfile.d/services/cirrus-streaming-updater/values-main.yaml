# WMF production main release specific values.
#
# These are kept out of the primary values.yaml just so that it is easier to use
# with local development.
#
# Settings in this file should apply for all WMF production k8s clusters & enviroments.
# cluster/environment/release specific settings belong in env specific values files.

flink:
  object_store:
    swift_access_key: search-update-pipeline:prod
    swift_secret_key: secret
    # We can put this config in values-main.yaml because it uses discovery endpoint,
    # which will be resolved correctly depending on which DC we are deploying to.
    swift_cluster: thanos-swift.discovery.wmnet

app:
  job:
    upgradeMode: last-state

  flinkConfiguration:
    # Set to a constant port which will have trigger adding appropriate networkpolicy
    "metrics.internal.query-service.port": "50100"
    # Flink HA config shared across clusters.
    # Swift and Zookeeper specific configs (paths), should be defined
    # in each cluster's  helmfile.
    state.backend.type: hashmap
    state.checkpoints.num-retained: "10"
    # Enable automatic restarts
    restart-strategy.type: failure-rate
    restart-strategy.failure-rate.delay: "1 min"
    restart-strategy.failure-rate.failure-rate-interval: "2 min"
    restart-strategy.failure-rate.max-failures-per-interval: "10"
    # When HA is disabled we need to set an interval in order to trigger checkpointing.
    # TODO: after gaining some operational experience, set this value to something
    # meaningful. Given the small size of kafka offsets (KBs), 10 seconds seems a reasonable
    # default.
    execution.checkpointing.interval: "10000" # 10 seconds.
    # Have flink-operator periodic savepoints for handling job restarts and upgrades.
    kubernetes.operator.periodic.savepoint.interval: 5m
    # Keep the last 24 hours of savepoints.  Savepoint cleanup happens
    # only when the job is running, so if the job is offline e.g. over a weekend,
    # we won't going to lose the latest savepoint.
    kubernetes.operator.savepoint.history.max.age: 24h
    # Increase from default 5s to 30s, we've (esp. codfw) having troubles with repeated timeout failures
    # and since these are during writes they are not generally retried and thus causes the job to restart
    # (see T362508 for context on the first time it happened on another job)
    s3.socket-timeout: 30s

  config_files:
    app.config.yaml:
      # mw-api-int-async-ro
      http-routes.99-mwapi: .*=http://localhost:6500
      event-stream-http-routes.schema: https://schema.wikimedia.org=http://localhost:6023
      # mw-api-int-async-ro
      event-stream-http-routes.meta: https://meta.wikimedia.org=http://localhost:6500

# Enable egress.  Specific egress policies should either be added in
# environment/k8s cluster specific networkpolicy.egress.dst_nets,
# or automatically configured via discovery.listeners and/or kafka.allowed_clusters,
networkpolicy:
  egress:
    enabled: true

# Enable the service mesh.
mesh:
  enabled: true

# Open up communication to listed services
# Note that this is overridden for each consumer instance to add
# in the appropriate elasticsearch clusters
discovery:
  ratelimit_listeners:
    mw-api-int-async-ro:
      domain: mw-api-int
  listeners:
    - mw-api-int-async-ro
    - schema
    - thanos-swift # cluster for checkpoints
