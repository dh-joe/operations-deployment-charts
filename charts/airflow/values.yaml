# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
app:
  image: &base-image repos/data-engineering/airflow
  version: &base-image-version latest # we use latest everywhere in the defaults.
  executor_pod_image: *base-image
  executor_pod_image_version: *base-image-version
  port: &airflow_frontend 8080 # airflow webUI
  # port: *airflow_frontend # port exposed as a Service, also used by service-checker.
  # Use command and args below to override the entrypoint. Type is arrays
  # Not necessary unless you want to change the entrypoint defined in the docker image
  # Example:
  # command: ["node"]
  # args: ["bin/server.js", "--param1", "arg1"]
  command: ["airflow"]
  args: ["webserver", "--pid" , "/tmp/airflow-webserver.pid"]
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 1000m
    memory: 2Gi
  liveness_probe:
    tcpSocket:
      port: *airflow_frontend
  readiness_probe:
    httpGet:
      path: /health
      port: *airflow_frontend

  volumes:
  - name: airflow-config
    configMap:
      name: airflow-config
  - name: airflow-webserver-config
    configMap:
      name: airflow-webserver-config
  - name: airflow-bash-executables
    configMap:
      name: airflow-bash-executables
      defaultMode: 0777
  - name: airflow-connections-variables
    secret:
      secretName: airflow-connections-variables
  - name: airflow-kubernetes-executor-pod-template
    configMap:
      name: airflow-kubernetes-executor-pod-template
  - name: logs
    emptyDir: {}
  - name: airflow-dags
    persistentVolumeClaim:
      claimName: airflow-dags-pvc

  volumeMounts: &airflow_volume_mounts
  - name: airflow-config
    mountPath: /opt/airflow/airflow.cfg
    subPath: airflow.cfg
  - name: airflow-webserver-config
    mountPath: /opt/airflow/webserver_config.py
    subPath: webserver_config.py
  - name: logs
    mountPath: /opt/airflow/logs
  - name: airflow-bash-executables
    mountPath: /opt/airflow/usr/bin
  - name: airflow-connections-variables
    mountPath: /opt/airflow/secrets
  - name: airflow-dags
    readOnly: true
    mountPath: &dags_root /opt/airflow/dags
  - name: airflow-kubernetes-executor-pod-template
    mountPath: &pod_template_file /opt/airflow/pod_templates/pod_template.yaml
    subPath: pod_template.yaml

service:
  deployment: minikube # valid values are "production" and "minikube"
  port:
    name: http # a unique name of lowercase alphanumeric characters or "-", starting and ending with alphanumeric, max length 63
    # protocol: TCP # TCP is the default protocol
    targetPort: *airflow_frontend # the number or name of the exposed port on the container
    port: *airflow_frontend # the number of the port desired to be exposed to the cluster
    nodePort: null # you need to define this if "production" is used. In minikube environments let it autoallocate

config:
  public: # Add here all the keys that can be publicly available as a ConfigMap
    AIRFLOW_HOME: /opt/airflow
  private: {} # Add here all the keys that should be private but still available as env variables
  airflow:
    postgresqlPass: secret
    dbHost: ''
    dbName: ''
    dbUser: ''
    dags_root: *dags_root
    dags_folder: override_me
    instance_name: override_me
    aws_access_key_id: override_me
    aws_secret_access_key: override_me
    s3_bucket: &s3_bucket '{{ printf "logs.airflow-%s.%s" (replace "_" "-" $.Values.config.airflow.instance_name) $.Values.environmentName }}'

    auth:
      role_admin: Admin
      roles_sync_at_login: true
      user_registration: true
      user_registration_role: Public
      role_mappings:
        nda: [User]
        wmf: [User]
        ops: [Admin]

    # https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html
    config:
      core:
        colored_console_log: false
        dags_folder: "{{ $.Values.config.airflow.dags_root }}/{{ $.Values.gitsync.link_dir }}/{{ $.Values.config.airflow.dags_folder }}"
        plugins_folder: "{{ $.Values.config.airflow.dags_root }}/{{ $.Values.gitsync.link_dir }}/wmf_airflow_common/plugins"
        executor: LocalExecutor
        load_examples: false
        max_active_runs_per_dag: 3
        max_active_tasks_per_dag: 6
        default_task_retries: 5
        parallelism: 64
        test_connection: enabled
      database:
        load_default_connections: false
      kubernetes_executor:
        multi_namespace_mode: false
        namespace: "{{ $.Release.Namespace }}"
        pod_template_file: *pod_template_file
        worker_container_repository: "{{ $.Values.docker.registry }}/{{ $.Values.app.executor_pod_image }}"
        worker_container_tag: "{{ $.Values.app.executor_pod_image_version }}"
      logging:
        colored_console_log: false
        remote_logging: true
        delete_local_logs: true
        remote_base_log_folder: 's3://{{ $.Values.config.airflow.s3_bucket }}'
        remote_log_conn_id: s3_dpe
        encrypt_s3_logs: false
      metrics:
        metrics_allow_list: operator_failures_,operator_successes_,sla_missed,executor.queued_tasks,dag.,dagrun.duration.,scheduler.scheduler_loop_duration,dag_processing.import_errors,dag_processing.total_parse_time,ti.failures,ti.successes,ti.finish,ti_failures,ti_successes
        statsd_custom_client_path: wmf_airflow_common.metrics.custom_statsd_client.CustomStatsClient
        statsd_host: localhost
        statsd_on: True
        statsd_port: "{{ $.Values.monitoring.statsd.port }}"
        statsd_prefix: airflow
      scheduler:
        standalone_dag_processor: false
        enable_health_check: true
        scheduler_health_check_server_host: '0.0.0.0'
        scheduler_health_check_server_port: &scheduler_healthcheck_port 8765
      secrets:
        backend: airflow.secrets.local_filesystem.LocalFilesystemBackend
        # See https://github.com/apache/airflow/blob/73dd6c17bd10ddda63a1682ac2174b0d206590dd/airflow/secrets/local_filesystem.py#L278-L279
        backend_kwargs: '{"connections_file_path": "/opt/airflow/secrets/connections.yaml", "variables_file_path": "/opt/airflow/secrets/variables.yaml"}' # WARN: must be json-parseable
      email:
        default_email_on_retry: false # T377745. We want to be alerted only after exhausting all retries.
      smtp:
        smtp_host: mx-out1001.wikimedia.org
        smtp_mail_from: "Airflow dse-k8s-eqiad: <noreply@wikimedia.org>"
        smtp_port: 25
        smtp_ssl: false
        smtp_starttls: false
      triggerer:
        default_capacity: 1000
      webserver:
        enable_proxy_fix: true
        rbac: true
        base_url: "https://{{ $.Values.ingress.gatewayHosts.default }}.wikimedia.org"
        # Our deployments are public-facing by default, but secured by OIDC authentication. Therefore, we can disable this warning. See #T375739
        warn_deployment_exposure: false
        expose_config: non-sensitive-only
        instance_name: "{{ $.Values.config.airflow.instance_name }}"
        expose_hostname: true
        expose_stacktrace: true

  # https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html
  connections:
    fs_local:
      conn_type: fs
      description: Local filesystem on the Airflow Scheduler node
    s3_dpe:
      conn_type: aws
      login: "{{ $.Values.config.airflow.aws_access_key_id }}"
      password: "{{ $.Values.config.airflow.aws_secret_access_key }}"
      extra:
        verify: /etc/ssl/certs/wmf-ca-certificates.crt

  # https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html
  variables:
    s3_log_bucket: *s3_bucket
    s3_log_retention_days: 30

  oidc:
    idp_server: override_me
    client_id: override_me
    client_secret: override_me

scheduler:
  enabled: true
  service_name: airflow-scheduler
  local_executor_api_port: 8793
  liveness_probe:
    tcpSocket:
      port: *scheduler_healthcheck_port
  readiness_probe:
    httpGet:
      path: /health
      port: *scheduler_healthcheck_port
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 1000m
    memory: 2Gi
  volumeMounts: *airflow_volume_mounts

# Export the content of the postgresql app secret to environment variables
postgresql:
  cloudnative: true
  secrets:
    uri: PG_URI
    host: PG_HOST

gitsync:
  enabled: true
  image_name: repos/data-engineering/git-sync
  image_tag: override_me
  image_gid: 900
  dags_repo: "https://gitlab.wikimedia.org/repos/data-engineering/airflow-dags.git"
  root_dir: /dags
  link_dir: airflow_dags
  ref: main
  period: 300 # git pull every 5 minutes
  volume:
    storage_class: override_me
    size: 100Mi

worker:
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 2000m
    memory: 4Gi

# Additional resources if we want to add a port for a debugger to connect to.
debug:
  enabled: false
  # Define here any port that you want to expose for debugging purposes
  ports: []

# Allow external traffic to reach this service via a (cluster provided) ingress controller.
# https://wikitech.wikimedia.org/wiki/Kubernetes/Ingress#Configuration_(for_service_owners)
ingress:
  enabled: true
  # By default, enabling ingress will switch the charts services from type NodePort to
  # ClusterIP. While that is fine for new services it may not be desired during transition
  # of existing ones from dedicated LVS to Ingress.
  # By setting keepNodePort to true, the services will stay of type NodePort.
  keepNodePort: false
  # gatewayHosts settings configure the hostnames this service will be reachable on.
  # By default, this will be a list like:
  # - {{ gatewayHosts.default }}.{{ domain }}
  # For all domains listed in .gatewayHosts.domains (specified by SRE for each environment)
  gatewayHosts:
    # default will expand to {{ .Release.Namespace }} as long as it is an empty string.
    default: ""
    # disableDefaultHosts may be set to true if the service should not be reachable via
    # the gateway hosts generated by default (see above).
    disableDefaultHosts: false
    # extraFQDNs ist a list of extra FQDNs this service should be reachable on.
    # It can be used to extend the gateway hosts that are generated by default.
    extraFQDNs: []
  # If you want to attach routes of this release to an existing Gateway, provide the name
  # of that gateway here in the format: <namespace>/<gateway-name>
  # This is useful if you wish to make multiple releases available from the same hostname.
  existingGatewayName: ""
  # routeHosts is a list of FQDNs the httproutes should attach to.
  # If existingGatewayName not set, this list might be empty and will default to the gateway
  # host generated according to how .Values.gatewayHosts.* is configured.
  # If existingGatewayName is set, you need to provide the FQDNs your routes should attach to.
  routeHosts: []
  # HTTPRoute routing rules. By default https://<hosts from above>/ will be routed to
  # the service without modification.
  # Docs: https://istio.io/v1.9/docs/reference/config/networking/virtual-service/#HTTPRoute
  httproutes: []
  # Base CORS HTTP headers for the general use case.
  base_cors_policy: false
  # Add a custom CORS policy, injecting an Istio CorsPolicy config:
  # https://istio.io/latest/docs/reference/config/networking/virtual-service/#CorsPolicy
  # Takes precedence over base_cors_policy.
  custom_cors_policy: {}

# Basic mesh-related data.
mesh:
  enabled: true
  certmanager:
    enabled: true
  admin: {port: 1666 }
  image_version: latest
  # http keepalive timeout for incoming requests
  idle_timeout: "4.5s"
  # Port to listen to
  public_port: 1025 # arbitrary placeholder
  local_access_log_min_code: "200"
  # Headers to add to a local request,
  # in dictionary form.
  request_headers_to_add: []
  # Timeout of a request to the local service
  upstream_timeout: "60s"
  # Enabling telemetry, telemetry port.
  telemetry:
    enabled: true
    port: 1667
  resources:
    requests:
      cpu: 200m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

# Mesh-related discovery
# TODO: move under mesh.* once we can
discovery:
  # List of listeners
  listeners: []

# Mesh related pure TCP proxies
tcp_proxy:
  listeners: []

# Should be provided by configuration management.
# See details of the structures in the comments
# In the configuration module.
services_proxy: ~
tcp_services_proxy: ~

common_images:
  statsd:
    exporter: prometheus-statsd-exporter:latest
# WARNING: If you want to enable the module,
# you will need to add a "statsd" stanza to monitoring
# see modules/base/values.yaml for reference.
  kerberos:
    image: repos/data-engineering/kerberos-kinit
    version: latest

docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent
resources:
  replicas: 1

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: true
  prometheus_port: 9102
  statsd:
    port: 9125
    prestop_sleep: 0
    requests:
      memory: 100Mi
      cpu: 100m
    limits:
      memory: 200Mi
      cpu: 200m
    # from https://github.com/blueswen/gunicorn-monitoring
    config: |-
      mappings:
      - match: airflow.scheduler.scheduler_loop_duration
        name: airflow_scheduler_loop_duration
        summary_options:
          quantiles:
            - quantile: 0.5
              error: 0.1
          max_age: 30s
          age_buckets: 30
          buf_cap: 10

      # Example:
      #   airflow.operator_failures__HdfsEmailOperator count
      #   => airflow_operator{operator="HdfsEmailOperator", state="failures"} count
      - match: airflow\.operator_(failures|successes)[\._]+(\w+)
        match_type: regex
        name: airflow_operator
        ttl: 1m
        labels:
          state: "$1"
          operator: "$2"

      # Example:
      #   airflow.ti_failures count
      #   => airflow_ti{state="failures"} count
      - match: airflow\.ti_(failures|successes)
        match_type: regex
        name: airflow_ti
        labels:
          state: "$1"

      # Example:
      #   airflow.ti.start.example_python_operator.print_array count
      #   dropped
      - match: airflow\.ti\.start.*
        match_type: regex
        name: airflow_ti_start
        action: drop


      # Renamed to remove ambiguity with the next mapping
      # The ambiguity only happens in prod, and depends on the order of the metric arrivals to the exporter.
      - match: airflow\.ti\.finish$
        match_type: regex
        name: airflow_ti_finish_aggregated

      # Example:
      #   airflow.ti.finish.example_dag.task_1.failed count
      #   => airflow_ti_finish{dag_id="example_dag", task_id="task1", state="failed"} count
      - match: airflow\.ti\.finish.(\w+)\.(\w+)\.(queued|running|scheduled|success|failed)
        match_type: regex
        name: airflow_ti_finish
        ttl: 1m
        labels:
          dag_id: "$1"
          task_id: "$2"
          state: "$3"

      # Example:
      #   airflow.ti.finished.example_python_operator.print_array.None count
      #   dropped
      - match: airflow\.ti\.finish\.(\w+)\.(\w+)\.(None|deferred|removed|restarting|shutdown|skipped|up_for_reschedule|up_for_retry|upstream_failed)
        match_type: regex
        name: airflow_ti_finish_useless
        action: drop

      # Example:
      #   airflow.dag.pageview_hourly.move_data_to_archive.duration
      #   => airflow_dag_duration{dag_id="pageview_hourly", task_id="move_data_to_archive"} count
      - match: airflow\.dag\.(\w+)\.(\w+)\.duration
        match_type: regex
        name: "airflow_task_duration"
        ttl: 1m
        labels:
          dag_id: "$1"
          task_id: "$2"

      # Example:
      #   airflow.dag.pageview_hourly.move_data_to_archive.duration
      #   dropped
      - match: airflow\.dag\.(\w+)\.(\w+)\.(queued_duration|scheduled_duration)
        match_type: regex
        name: airflow_dag_other_durations
        action: drop

      # Example:
      #   airflow.dagrun.duration.pageview_hourly count
      #   dropped
      - match: airflow\.dagrun\.duration\.(success|failed)$
        match_type: regex
        name: airflow_dagrun_duration_success
        action: drop

      # Example:
      #   airflow.dagrun.duration.success.pageview_hourly count
      #   => airflow_dagrun_duration{dag_id="pageview_hourly", state="success"} count
      - match: airflow\.dagrun\.duration\.(success|failed)\.(\w+)
        match_type: regex
        name: airflow_dagrun_duration
        ttl: 1m
        labels:
          state: "$1"
          dag_id: "$2"

networkpolicy:
  egress:
    enabled: true

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

kerberos:
  enabled: false
  # ticket_renewal_interval_minutes: 60 # although we have a `default` value, so not 100% mandatory
  # keytab: override_me
  # resources:
  #   requests:
  #     cpu: 100m
  #     memory: 200Mi
  #   limits:
  #     cpu: 500m
  #     memory: 400Mi

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# Cronjob definitions
# Here you can define your cronjobs
cronjobs: []
# Example of a job:
# - name: my-cron-hourly
#   enabled: true
#   command:
#      - /bin/cowsay
#      - "hello"
#   schedule: "@hourly" (defaults to @daily)
#   concurrency: Replace (defaults to "Forbid")
#   image_versioned: my-app:1.1.1 (defaults to the app used in the main application definition)
#   resources: (optional list of requests/limits for our cronjob; if not present will default to the application ones.)

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services:
  gitlab: [wikimedia]
  wikimail: [mx]

pgServiceName: override_me
environmentName: override_me
