This is a simple Helm chart that wraps the big yaml file that Kserve upstream
provides (basically model-serving only).

The related InferenceService resources are managed by the kserve-inference
chart, this one only takes care of the base KServe setup.

== TLS certificates ==

There is one TLS certificate that is mandatory for this chart, namely the one
used by the Webhook (upgrade/compatibility between api versions, admission, etc..).
The TLS certificate needs to placed into kserve-webhook-server-cert (see secret.yaml),
using the following (default) CN/SANS:
- CN: kserve-webhook-server-service.kserve.svc.cluster.local
- SANs: kserve-webhook-server-service.kserve.svc,
        kserve-webhook-server-service.kserve.svc.cluster,
        kserve-webhook-server-service.kserve.svc.cluster.local
Please also note that for the moment the target namespace, kserve, needs
to be mentioned in the CN and SANs.

There Certificate is currently provisioned via cert-manager. We previously
supported a custom Secret and the Puppet CA, but not anymore.

When importing a new version please check the cert-manager code that we use,
since it contains some conditionals to support both use cases.

== Inference Services ==

This chart provides the definition of the InferenceService CRD, that allows
the deployment of models and their settings to serve them via Istio/Knative.
The definition of the InferenceService endpoints will be delegated to a separate
chart, kserve-inference, this one will be used only to spin up
the base KServe service and configuration.

== Upgrading ==

The kube-rbac-proxy image is used to add a proxy in front of Prometheus metrics,
usually with something like:
```
- args:
  - --secure-listen-address=0.0.0.0:8443
  - --upstream=http://127.0.0.1:8080/
  - --logtostderr=true
  - --v=10
  image: gcr.io/kubebuilder/kube-rbac-proxy:v0.4.0
  name: kube-rbac-proxy
  ports:
  - containerPort: 8443
    name: https
```
We don't really need it, so it can be safely removed. All the occurrences of port
8443 in the yaml file provided by upstream should be replaced with
{{ $.Values.kserve.controller.metrics.port }} (defaults to 8080 in values.yaml).
Remember also to set the following for the args of the controller:
```
--metrics-addr=0.0.0.0:{{ $.Values.kserve.controller.metrics.port }}
```

We also move the prometheus annotations from the Service definition to
the container definition, since our Prometheus master config automatically
pulls annotations for pods/containers, not services.

The chart supports cert-manager so the following snippet is customized for
our use case (for example, we don't need the self-signed issuer since we have
our own intermediate CA to use):
```
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: serving-cert
  namespace: kserve-system
spec:
  commonName: kserve-webhook-server-service.kserve.svc
  dnsNames:
  - kserve-webhook-server-service.kserve.svc
  issuerRef:
    kind: Issuer
    name: selfsigned-issuer
  secretName: kserve-webhook-server-cert
```

The following snipped can be removed (not used due to what written above):
```
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: selfsigned-issuer
  namespace: kserve
spec:
  selfSigned: {}
```

The annotations like the following are added with conditionals in our code
(namely if we use or not cert-manager, see above):

```
cert-manager.io/inject-ca-from: kserve/serving-cert
```

The path for the go binary to run for the KServe controller needs to be changed
from /manager to /usr/bin/manager.

With 0.6 a web-app was introduced, but this chart currently doesn't support it.
Clean up all resources with "kserve-models-web-app" accordingly.

Please also add the following snippet to the `env` specs of all containers:
```
{{- if and .Values.kubernetesApi.host .Values.kubernetesApi.port }}
{{- include "wmf.kubernetes.ApiEnv" . | nindent 12 }}
{{- end }}
```
This is needed to avoid TLS certificate validation errors due to the absence of IP SANs.

We also modify the `inferenceservice-config` configmap with:
```
ingress: |-
  {
      "ingressGateway" : "knative-serving/knative-ingress-gateway",
      "ingressService" : "istio-ingressgateway.istio-system.svc.cluster.local",
      "localGateway" : "knative-serving/knative-local-gateway",
      "localGatewayService" : "{{ $.Values.ingress.local_gateway_service }}"
  }
```
We need to set `cluster-local-gateway.istio-system.svc.cluster.local` with Knative
<= 0.18 (from 0.19+ there is no need due to the knative-local-gateway).

The Namespace creation for `kserve` is delegated to helmfile's admin_ng
config, so when importing a new version it needs to be removed. Important: check
the labels defined for the namespace in helmfile vs the ones shipped by upstream,
and correct our settings in case they are different.

The webhook port is not fixed but it is a variable of the template, in order
to re-use it for network policies:
```
- containerPort: {{ $.Values.webhook.port }}
  name: webhook-server
```

We add the following labels to the pod template of the Controller's StatefulSet
definition:
```
app-wmf: {{ template "wmf.chartname" . }}
chart: {{ template "wmf.chartid" . }}
release: {{ .Release.Name }}
```
We use the `app-wmf` label in our charts since the `app` label is already used
by kserve. The `app-wmf` value is useful to deploy network policies
safely and consistently.

From release 0.8+ we removed occurrences of `creationTimestamp: null` since our
CI yaml validators complain about them.

From release 0.8+ there is a new yaml offered by upstream, called `kserve-runtimes.yaml`,
that should not be needed since we currently build our own images/runtimes, so
it wasn't imported.
