This is a simple Helm chart to instantiate the InferenceService resources
needed by KServe to create services.

Every inference_services entry will correspond to an InferenceService resource,
that will be composed by the base/common config stated in "inference" plus some
custom one.

Please check the .fixtures directory to get some examples of how to configure
this chart.

The fact that there may be multiple InferenceService for the same chart is related
to how models will be grouped together. For example, we may want to deploy all
the ORES-related models (100+) in the same namespace (like inference-ores),
meanwhile other models (say for example image recognition, etc..) in another
namespace. The idea is to control this via helmfile, having multiple releases
based on the groups of models to deploy.
Istio will take care of doing the http(s) proxy between external clients and models
using the "Host" header passed by the client.

We use the `app-wmf` label in our charts since the `app` label is already used
by knative-serving, that sets it equal to the revision name. The `app-wmf` value
is useful to deploy network policies safely and consistently.