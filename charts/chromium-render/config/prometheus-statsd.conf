# Note: statsd timer metrics are millisecond based, but prometheus-statsd-exporter makes them
# second based to conform to prometheus, so all metrics are divided by 1000
mappings:
  - match: '*.gc.*'
    name: service_runner_gc_seconds
    timer_type: histogram
    buckets: [5e+2, 1e+3, 5e+3, 1e+4, 5e+4, 1e+5, 5e+5, 1e+6]
    labels:
      service: $1
      event: $2

  - match: '*.heap.*'
    # service-runner abuses timer for heap data
    name: service_runner_${2}_heap_kilobytes
    timer_type: histogram
    buckets: [1e+3, 1e+4, 1e+5, 1e+6, 1e+7]
    labels:
      service: $1

# This will match standard service-runner request metrics of the form:
#    <service_name>.<endpoint>.<method>.<http_status>, e.g.:
#    mathoid.-info.GET.200

  - match: '^([a-z0-9-]+)\.([^.]+)\.(GET|HEAD|POST|PUT|DELETE|CONNECT|OPTIONS|TRACE|PATCH)\.([0-9][0-9][0-9]|ALL|[0-9]xx)$'
    match_type: regex
    name: service_runner_request_duration_seconds
    timer_type: histogram
    # chromium-render has rather high latencies, adjust for that
    buckets:  [0.1, 0.5, 1, 5, 10, 30, 60]
    labels:
      service: $1
      uri: $2
      method: $3
      status: $4

    # Some proton specific metrics forced to histograms instead of summaries
  - match: '*.job.render_time'
    name: chromium_render_job_render_time
    timer_type: histogram
    buckets:  [0.1, 0.5, 1, 5, 10, 30, 60]
    labels:
      service: $1

  - match: '*.job.wait_time'
    name: chromium_render_job_wait_time
    timer_type: histogram
    buckets:  [0.1, 0.5, 1, 5, 10, 30, 60]
    labels:
      service: $1
