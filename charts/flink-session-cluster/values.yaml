# Default values for rdf-streaming-updater.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
helm_scaffold_version: 0.1 # This can be useful when backporting fixes.
docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent

main_app:
  # Create a dedicated flink image instead of using this one
  image: wikimedia/wikidata-query-flink-rdf-streaming-updater
  version: latest
  replicas: 1
  # REST endpoint, must be the same as .Values.service.port.targetPort (used to feed the rest.port in the flink conf)
  # This is needed by the tls template
  port: 8081
  requests:
    cpu: 100m
    memory: 700Mi
  limits:
    cpu: 200m
    memory: 800Mi
  liveness_probe:
    tcpSocket:
      port: 6123
    initialDelaySeconds: 30
    periodSeconds: 60
  config:
    task_slots: 2
    job_manager_mem : 600m
    task_manager_mem: 1000m
    blob_server_port: 6124
    jobmanager_rpc_port: 6123
    taskmanager_rpc_port: 6122
    taskmanager_data_port: 6121
    queryable_state_proxy_port: 6125
    prometheus_reporter_port: 9102
    parallelism: 1
    swift_auth_url: "https://thanos-swift.discovery.wmnet/auth/v1.0"
    swift_username: "wdqs:flink"
    swift_s3_endpoint: thanos-swift.discovery.wmnet
    # the cluster id used by flink H/A components
    # see https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/config/#kubernetes-cluster-id
    # the cluster id is used by flink to name some k8s resources it creates like h/a configmaps
    # this name must unique in the k8s namespace but it's advised to make it globally unique in the infrastructure
    # to avoid ambiguities.
    cluster_id: flink-cluster
    # storage used by flink's H/A components, it's required to recover running jobs when the jobmanager fails
    # unexpectedly.
    # It must be overridden and set with a durable object storage like swift and not a local folder like here (used for testing only)
    ha_storage_dir: file:///streaming_updater/ha_storage_dir
    additional_flink_settings: {}
    logging_level: INFO

task_manager:
  replicas: 1
  requests:
    cpu: 100m
    memory: 1000Mi
  limits:
    cpu: 200m
    memory: 1100Mi
  liveness_probe:
    tcpSocket:
      port: 6122
    initialDelaySeconds: 30
    periodSeconds: 60

service:
  deployment: minikube # valid values are "production" and "minikube"
  port:
    name: http # a unique name of lowercase alphanumeric characters or "-", starting and ending with alphanumeric, max length 63
    # protocol: TCP # TCP is the default protocol
    targetPort: 8081 # the number or name of the exposed port on the container
    port: 8081 # the number of the port desired to be exposed to the cluster
    nodePort: null # you need to define this if "production" is used. In minikube environments let it autoallocate
config:
  public: {} # Add here all the keys that can be publicly available as a ConfigMap
  private:
    swift_api_key: some_secret_key

mesh:
  enabled: false # Switch to true in production
  # image_version: 1.15.1-2 # image_version is defined globally by SRE. May be overridden here, though.
  public_port: 8081 # the port where TLS will be exposed
  upstream_timeout: "180.0s"
  # To be defined in a private space
  certs:
    cert: "snakeoil"
    key: "snakeoil"
  # Enable telemetry
  telemetry:
    enabled: true
    port: 9361
# Add here, via our "secret" system, the cert/key pairs
#    cert: "your cert here"
#    key: "your key here"

# Additional resources if we want to add a port for a debugger to connect to.
debug:
  enabled: false
  # Define here any port that you want to expose for debugging purposes
  ports: []

networkpolicy:
  egress:
    enabled: false

app:
  port: 8081
