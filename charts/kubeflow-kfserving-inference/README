This is a simple Helm chart to instantiate the InferenceService resources
needed by Kubeflow's Kfserving to create services.

The idea is to configure values to something like:

docker:
  registry: docker-registry.wikimedia.org/wikimedia
  imagePullPolicy: IfNotPresent

inference:
  image: "machinelearning-liftwing-inference-services-editquality"
  version: "2021-09-01-140944-production"
  annotations:
    sidecar.istio.io/inject: "false"
  labels:
    controller-tools.k8s.io: "1.0"
  base_env:
  - name: WIKI_URL
    value: https://api-ro.discovery.wmnet
  - name: REQUESTS_CA_BUNDLE
    value: /usr/share/ca-certificates/wikimedia/Puppet_Internal_CA.crt

inference_services:
  - name: enwiki-goodfaith
    predictor_config:
      minReplicas: 2
      canaryTrafficPercent: 10
    custom_env:
    - name: INFERENCE_NAME
      value: enwiki-goodfaith
    - name: WIKI_HOST
      value: en.wikipedia.org
    - name: STORAGE_URI
      value: s3://wmf-ml-models/goodfaith/enwiki/202105140814/
  - name: itwiki-goodfaith
    image_version: '42'
    image: 'some-nice-custom-image-different-from-default'
    custom_env:
    [..]

Every inference_services entry will correspond to an InferenceService resource,
that will be composed by the base/common config stated in "inference" plus some
custom one.

The fact that there may be multiple InferenceService for the same chart is related
to how models will be grouped together. For example, we may want to deploy all
the ORES-related models (100+) in the same namespace (like inference-ores),
meanwhile other models (say for example image recognition, etc..) in another
namespace. The idea is to control this via helmfile, having multiple releases
based on the groups of models to deploy.
Istio will take care of doing the http(s) proxy between external clients and models
using the "Host" header passed by the client.