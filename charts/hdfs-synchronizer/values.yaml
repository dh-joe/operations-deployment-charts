# Default values.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
app:
  image: python3-bullseye:0.0.2-20240929
  version: latest # we use latest everywhere in the defaults.
  port: 4567 # port exposed as a Service, also used by service-checker.
  # Use command and args below to override the entrypoint. Type is arrays
  # Not necessary unless you want to change the entrypoint defined in the docker image
  # Example:
  # command: ["node"]
  # args: ["bin/server.js", "--param1", "arg1"]
  command: []
  args: []
  requests:
    cpu: 100m # Just some sample, edit these - 100m is the minimum for deployment
    memory: 2000Mi # Just some sample, edit these
  limits:
    cpu: 1 # Just some sample, edit these
    memory: 2000Mi # Just some sample, edit these
  liveness_probe:
    tcpSocket:
      port: 4567
  readiness_probe:
    httpGet:
      path: /
      port: 4567
  # add here any volumes to mount onto the pod. Example:
  #volumes:
  #- name: scratchdir
  #  emptyDir: {}
  # add here any mounted volumes to make accessible to the container. Example:
  #volumeMounts:
  #   - name: scratchdir # the name from your volume above
  #     mountPath: /scratch/ # where it should live in your container
monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false

service:
  deployment: minikube # valid values are "production" and "minikube"
  port:
    name: http # a unique name of lowercase alphanumeric characters or "-", starting and ending with alphanumeric, max length 63
    # protocol: TCP # TCP is the default protocol
    targetPort: 4567 # the number or name of the exposed port on the container
    port: 4567 # the number of the port desired to be exposed to the cluster
    nodePort: null # you need to define this if "production" is used. In minikube environments let it autoallocate

config:
  public: {} # Add here all the keys that can be publicly available as a ConfigMap
  private: {} # Add here all the keys that should be private but still available as env variables

# Additional resources if we want to add a port for a debugger to connect to.
debug:
  enabled: false
  # Define here any port that you want to expose for debugging purposes
  ports: []

cronjobs: {}
#    my-cron-hourly:
#      name: my-cron-hourly
#      enabled: true
#      command:
#        - /bin/cowsay
#        - "hello"
#      schedule: "@hourly"
#      concurrency: Replace
#      image_versioned: my-app:1.1.1
#      resources:
#        requests:
#          cpu: 500m
#           memory: 200M
#        limits:
#          cpu: 500m
#          memory: 200M
#      volumeMounts:
#        - mountPath: "/tmp/acow"
#          name: tmp-cow-memory
#      volumes:
#        - name: tmp-cow-memory
#          emptyDir:
#            medium: Memory
#            sizeLimit: "10Gi"

# Allow external traffic to reach this service via a (cluster provided) ingress controller.
# https://wikitech.wikimedia.org/wiki/Kubernetes/Ingress#Configuration_(for_service_owners)
ingress:
  enabled: false
  # By default, enabling ingress will switch the charts services from type NodePort to
  # ClusterIP. While that is fine for new services it may not be desired during transition
  # of existing ones from dedicated LVS to Ingress.
  # By setting keepNodePort to true, the services will stay of type NodePort.
  keepNodePort: false
  # gatewayHosts settings configure the hostnames this service will be reachable on.
  # By default, this will be a list like:
  # - {{ gatewayHosts.default }}.{{ domain }}
  # For all domains listed in .gatewayHosts.domains (specified by SRE for each environment)
  gatewayHosts:
    # default will expand to {{ .Release.Namespace }} as long as it is an empty string.
    default: ""
    # disableDefaultHosts may be set to true if the service should not be reachable via
    # the gateway hosts generated by default (see above).
    disableDefaultHosts: false
    # extraFQDNs ist a list of extra FQDNs this service should be reachable on.
    # It can be used to extend the gateway hosts that are generated by default.
    extraFQDNs: []
  # If you want to attach routes of this release to an existing Gateway, provide the name
  # of that gateway here in the format: <namespace>/<gateway-name>
  # This is useful if you wish to make multiple releases available from the same hostname.
  existingGatewayName: ""
  # routeHosts is a list of FQDNs the httproutes should attach to.
  # If existingGatewayName not set, this list might be empty and will default to the gateway
  # host generated according to how .Values.gatewayHosts.* is configured.
  # If existingGatewayName is set, you need to provide the FQDNs your routes should attach to.
  routeHosts: []
  # HTTPRoute routing rules. By default https://<hosts from above>/ will be routed to
  # the service without modification.
  # Docs: https://istio.io/v1.9/docs/reference/config/networking/virtual-service/#HTTPRoute
  httproutes: []
  # Base CORS HTTP headers for the general use case.
  base_cors_policy: false
  # Add a custom CORS policy, injecting an Istio CorsPolicy config:
  # https://istio.io/latest/docs/reference/config/networking/virtual-service/#CorsPolicy
  # Takes precedence over base_cors_policy.
  custom_cors_policy: {}

# Basic mesh-related data.
mesh:
  enabled: false
  admin: {port: 1666 }
  image_version: latest
  # http keepalive timeout for incoming requests
  idle_timeout: "4.5s"
  # Port to listen to
  public_port: 4567
  local_access_log_min_code: "200"
  # Headers to add to a local request,
  # in dictionary form.
  request_headers_to_add: []
  # Timeout of a request to the local service
  upstream_timeout: "60s"
  # Enabling telemetry, telemetry port.
  telemetry:
    enabled: true
    port: 1667
  resources:
    requests:
      cpu: 200m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

# Mesh-related discovery
# TODO: move under mesh.* once we can
discovery:
  # List of listeners
  listeners: []

# Mesh related pure TCP proxies
tcp_proxy:
  listeners: []

# Should be provided by configuration management.
# See details of the structures in the comments
# In the configuration module.
services_proxy: ~
tcp_services_proxy: ~

common_images:
  statsd:
    exporter: prometheus-statsd-exporter:latest
# WARNING: If you want to enable the module,
# you will need to add a "statsd" stanza to monitoring
# see modules/base/values.yaml for reference.

cache:
  mcrouter:
    enabled: true
    # This is the route prefix that will be added by default
    # to all requests whose key doesn't begin with /
    port: 11213
    route_prefix: /default
    cross_region_timeout: 250
    cross_cluster_timeout: 100
    num_proxies: 3
    probe_timeout: 6000
    timeouts_until_tko: 3
    zone: foo
    resources:
      requests:
        cpu: 250m
        memory: 200M
      limits:
        cpu: 250m
        memory: 200M
    pools:
      - name: foo-pool
        servers:
          - 192.168.1.1
          - 192.168.1.2
        failover:
          - 192.168.1.54
          - 192.168.1.53
        zone: foo
      - name: bar-pool
        servers:
          - 192.168.2.1
          - 192.168.2.2
        failover:
          - 192.168.2.54
          - 192.168.2.53
    routes:
      # Route 1: simple standalone
      - route: /default
        pool: foo-pool
        failover_time: 0
      # Route 2: "replica"
      # Actually generates two routes,
      # * /replica/foo that reads and writes to the foo pools
      # * /replica/bar that reads from foo and writes to bar
      # Applications can write to both using /replica/*/ as a
      # prefix.
      - route: /replica/foo
        pool: foo-pool
        failover_time: 10
        replica:
          route: /replica/bar
          pool: bar-pool
      # Route 3: "warmup"
      # This route will try reading a key from the local pool, and
      # if it doesn't find it, it will contact the bar pool and store
      # the resulting key for the ttl time
      - route: /multilayer
        pool: bar-pool
        failover_time: 10
        warmup:
          pool: foo-pool
          ttl: 60
    exporter:
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
        limits:
          cpu: 500m
          memory: 200Mi

docker:
  registry: docker-registry.wikimedia.org
  pull_policy: IfNotPresent
resources:
  replicas: 1

monitoring:
  # If enabled is true, monitoring annotations will be added to the deployment.
  enabled: false

networkpolicy:
  egress:
    enabled: false

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Optional affinity settings
affinity: {}
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: some-key
#                operator: In
#                values:
#                  - some-value
#  nodeSelector:
#    node.kubernetes.io/some-key: some-value

# The set of external services to allow egress to
# Example:
# kafka:
# - main-codfw
# - main-eqiad
# presto:
# - analytics
#
# See https://wikitech.wikimedia.org/wiki/Kubernetes/Deployment_Charts#Enabling_egress_to_services_external_to_Kubernetes
# for the list of supported services
external_services: {}
