apiVersion: crd.projectcalico.org/v1
kind: NetworkPolicy
metadata:
  name: {{ template "wmf.releasename" . }}-autoscaler-calico-ingress
  namespace: knative-serving
  labels:
    app: {{ template "wmf.chartname" . }}
    chart: {{ template "wmf.chartid" . }}
    release: {{ .Release.Name }}
spec:
  types:
    - Ingress
  selector: app.kubernetes.io/component=="autoscaler"
  # The Activator pod reports metrics to the Autoscaler when buffering
  # requests for scale-to-zero deployments.
  # https://github.com/knative/serving/blob/main/docs/scaling/SYSTEM.md
  # For more info about how ports are used:
  # kubectl describe service autoscaler -n knative-serving
  ingress:
    - action: Allow
      protocol: TCP
      destination:
        ports:
        - 8080
---
apiVersion: crd.projectcalico.org/v1
kind: NetworkPolicy
metadata:
  name: {{ template "wmf.releasename" . }}-autoscaler-calico-egress
  namespace: knative-serving
  labels:
    app: {{ template "wmf.chartname" . }}
    chart: {{ template "wmf.chartid" . }}
    release: {{ .Release.Name }}
spec:
  types:
    - Egress
  selector: app.kubernetes.io/component=="autoscaler"
  egress:
    - action: Allow
      destination:
        services:
          name: kubernetes
          namespace: default
    # The Autoscaler pod needs to be able to fetch Prometheus metrics from
    # all InferenceService pods to apply scaling policies correctly.
    - action: Allow
      protocol: TCP
      destination:
        selector: app-wmf=="kserve-inference"
        ports:
          - 9090
