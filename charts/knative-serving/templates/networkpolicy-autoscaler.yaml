apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: {{ template "base.name.release" . }}-autoscaler
  labels:
    app: {{ template "base.name.chart" . }}
    chart: {{ template "base.name.chartid" . }}
    release: {{ .Release.Name }}
spec:
  podSelector:
    matchLabels:
      app: autoscaler
      app-wmf: {{ template "base.name.chart" . }}
      chart: {{ template "base.name.chartid" . }}
      release: {{ .Release.Name }}
  policyTypes:
    - Ingress
{{- if .Values.networkpolicy.egress.enabled }}
    - Egress
  egress:
      # The Autoscaler pod needs to be able to fetch Prometheus metrics from
      # all InferenceService pods to apply scaling policies correctly.
    - to:
      - podSelector:
          matchLabels:
            app-wmf: kserve-inference
      ports:
        - port: 9090
          protocol: TCP
{{- if .Values.kubernetesMasters }}
    # Allow to query the K8s API.
    - to:
      {{- range $cidr := .Values.kubernetesMasters.cidrs }}
      - ipBlock:
          cidr: {{ $cidr }}
      {{- end }}
      ports:
        - port: 443
          protocol: TCP
        - port: 6443
          protocol: TCP
{{- end }}
{{- end }}
  ingress:
      # The Activator pod reports metrics to the Autoscaler when buffering
      # requests for scale-to-zero deployments.
      # https://github.com/knative/serving/blob/main/docs/scaling/SYSTEM.md
      # For more info about how ports are used:
      # kubectl describe service autoscaler -n knative-serving
    - from:
      - podSelector:
          matchLabels:
            app: activator
            app-wmf: {{ template "base.name.chart" . }}
            chart: {{ template "base.name.chartid" . }}
            release: {{ .Release.Name }}
      ports:
        - port: 8080
          protocol: TCP
