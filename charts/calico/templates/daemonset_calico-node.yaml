# This manifest installs the calico-node container on
# each worker node in a Kubernetes cluster.
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
    {{- include "calico.labels" . | nindent 4 }}
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
      {{- include "calico.selectorLabels" . | nindent 6 }}
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node
        {{- include "calico.labels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: calico-node
      hostNetwork: true
      priorityClassName: system-node-critical
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        # Make sure calico-node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
      # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
      containers:
        # Runs calico-node container on each Kubernetes node. This
        # container programs network policy and routes on each
        # host.
        - name: calico-node
          image: "{{ .Values.image.repository }}/{{ .Values.calicoNode.imageName }}:v{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
            # Don't try to manage the CNI's kubeconfig/token (puppet does that)
            - name: CALICO_MANAGE_CNI
              value: "false"
            # Use Kubernetes API as the backing datastore.
            - name: DATASTORE_TYPE
              value: "kubernetes"
            # Typha support.
            - name: FELIX_TYPHAK8SSERVICENAME
              value: "calico-typha"
            # Wait for the datastore.
            - name: WAIT_FOR_DATASTORE
              value: "true"
            # Set based on the k8s node name.
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Choose the backend to use.
            - name: CALICO_NETWORKING_BACKEND
              value: "bird"
            # Cluster type to identify the deployment type
            - name: CLUSTER_TYPE
              value: "k8s,bgp"
            # Auto-detect the BGP IP address.
            - name: IP
              value: "autodetect"
            # Auto-detect the BGP IPv6 address.
            - name: IP6
              {{- if .Values.IPv6Support }}
              value: "autodetect"
              {{- else }}
              value: "none"
              {{- end }}
            # Prevent Calico from creating a default pool if one does not exist.
            # This means we don't need all the CALICO_IPV(4|6)_ variables the upstream chart uses
            - name: NO_DEFAULT_POOLS
              value: "true"
            # Disable file logging so `kubectl logs` works.
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            # Set Felix endpoint to host default action to ACCEPT.
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: "ACCEPT"
            - name: FELIX_IPV6SUPPORT
              value: "{{ .Values.IPv6Support }}"
            - name: FELIX_HEALTHENABLED
              value: "true"
            # Since calico-node/Felix is host-networked, this opens a port on the host.
            # Extra scrape targets are in place in prometheus.
            - name: FELIX_PROMETHEUSMETRICSENABLED
              value: "true"
            - name: FELIX_PROMETHEUSMETRICSPORT
              value: "9091"
            # Have felix set the src parameter in routes it creates to so that
            # source address selection is forced. That way liveness/readiness
            # probes will originate from the node's IP
            - name: FELIX_DEVICEROUTESOURCEADDRESS
              valueFrom:
                fieldRef:
                  # Note that we can use status.podIP or status.hostIP here
                  # interchangeable. Since calico-node is a Daemonset it
                  # doesn't matter, but we use hostIP to be explicit
                  fieldPath: status.hostIP
            # Location of the CA bundle Felix uses to authenticate Typha; volume mount
            #- name: FELIX_TYPHACAFILE
            #  value: /calico-typha-ca/typhaca.crt
            # Common name on the Typha certificate; used to verify we are talking to an authentic typha
            #- name: FELIX_TYPHACN
            #  value: calico-typha
            # Location of the client certificate for connecting to Typha; volume mount
            #- name: FELIX_TYPHACERTFILE
            #  value: /calico-node-certs/calico-node.crt
            # Location of the client certificate key for connecting to Typha; volume mount
            #- name: FELIX_TYPHAKEYFILE
            #  value: /calico-node-certs/calico-node.key
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/calico-node
                - -shutdown
          livenessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-live
              - -bird-live
            periodSeconds: 10
            initialDelaySeconds: 10
            failureThreshold: 6
            timeoutSeconds: 10
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-ready
              - -bird-ready
            periodSeconds: 10
            timeoutSeconds: 10
          securityContext:
            privileged: true
          resources:
            {{- toYaml .Values.calicoNode.resources | nindent 12 }}
          volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
              readOnly: false
            - mountPath: /var/run/calico
              name: var-run-calico
              readOnly: false
            - mountPath: /var/lib/calico
              name: var-lib-calico
              readOnly: false
            - name: policysync
              mountPath: /var/run/nodeagent
            # For eBPF mode, we need to be able to mount the BPF filesystem at /sys/fs/bpf so we mount in the
            # parent directory.
            #- name: sysfs
            #  mountPath: /sys/fs/
            #  # Bidirectional means that, if we mount the BPF filesystem at /sys/fs/bpf it will propagate to the host.
            #  # If the host is known to mount that filesystem already then Bidirectional can be omitted.
            #  mountPropagation: Bidirectional
            # calico-node logs to /var/log/calico in addition to stdout
            # as we collect stdout anyways, avoid the extra writes to overlayfs by sending logs to emptyDir
            - mountPath: /var/log/calico
              name: var-log-calico
              readOnly: false
      volumes:
        # Used by calico-node.
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        - name: var-lib-calico
          hostPath:
            path: /var/lib/calico
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
        #- name: sysfs
        #  hostPath:
        #    path: /sys/fs/
        #    type: DirectoryOrCreate
        # Used to create per-pod Unix Domain Sockets
        - name: policysync
          hostPath:
            type: DirectoryOrCreate
            path: /var/run/nodeagent
        - name: var-log-calico
          emptyDir: {}
